{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe759bf-05e5-4c9e-a04c-1861f36e07e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "abe759bf-05e5-4c9e-a04c-1861f36e07e4",
    "outputId": "37ea1163-f840-437f-fb77-d7aa4ed346e0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import praw\n",
    "from pytrends.request import TrendReq\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from scipy import interpolate\n",
    "import feedparser\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d633fc-ebc5-4b0e-bfce-a035c0445ba1",
   "metadata": {
    "id": "93d633fc-ebc5-4b0e-bfce-a035c0445ba1"
   },
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def setup_environment():\n",
    "    nltk.download('punkt')\n",
    "    nltk.download9('stopwords')\n",
    "\n",
    "def load_stopwords(language='spanish'):\n",
    "    return set(stopwords.words(language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f42bf8-bec2-4527-b998-284803833192",
   "metadata": {
    "id": "b6f42bf8-bec2-4527-b998-284803833192"
   },
   "outputs": [],
   "source": [
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "    data.reset_index(inplace=True)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf95d5b-3764-48e4-bfae-577ea67ebfc0",
   "metadata": {
    "id": "7cf95d5b-3764-48e4-bfae-577ea67ebfc0"
   },
   "outputs": [],
   "source": [
    "def get_google_news_rss_feed(feed_url):\n",
    "    #Fetches articles from a google news RSS feed URL\n",
    "\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    if feed.bozo:\n",
    "        print(\"Error parsing feed:\", feed.bozo_exception)\n",
    "        return []\n",
    "    if not feed.entries:\n",
    "        print(\"No entries found in the feed.\")\n",
    "        return []\n",
    "    articles = []\n",
    "    for entry in feed.entries:\n",
    "        title = entry.title if 'title' in entry else 'No title'\n",
    "        summary = entry.summary if 'summary' in entry else 'No summary'\n",
    "        published = datetime(*entry.published_parsed[:6]) if 'published_parsed' in entry else None\n",
    "        articles.append({\n",
    "            'title': title,\n",
    "            'content': summary,\n",
    "            'date': published\n",
    "        })\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b08858e-5b8d-4ee3-833b-72f4663990c9",
   "metadata": {
    "id": "0b08858e-5b8d-4ee3-833b-72f4663990c9"
   },
   "outputs": [],
   "source": [
    "def get_reddit_posts(subreddits, query, limit=10):\n",
    "    posts = []\n",
    "    for subreddit in subreddits:\n",
    "        print(f\"Searching in subreddit: {subreddit}\")\n",
    "        subreddit_obj = reddit.subreddit(subreddit)\n",
    "        for submission in subreddit_obj.search(query, limit=limit):\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'content': submission.selftext,\n",
    "                'created': pd.to_datetime(submission.created_utc, unit='s')\n",
    "            })\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6f7cfa-60b3-49e3-90a4-c7193c88efe7",
   "metadata": {
    "id": "4a6f7cfa-60b3-49e3-90a4-c7193c88efe7"
   },
   "outputs": [],
   "source": [
    "def get_google_trends_data(keywords, timeframe='today 3-m'):\n",
    "    pytrends = TrendReq(hl='es-MX', tz=360)\n",
    "    pytrends.build_payload(keywords, cat=0, timeframe=timeframe, geo='MX', gprop='')\n",
    "    data = pytrends.interest_over_time()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2467b8ab-5229-4dad-8d56-4b3412ae1dcc",
   "metadata": {
    "id": "2467b8ab-5229-4dad-8d56-4b3412ae1dcc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text, stop_words):\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text) # Removes URLs\n",
    "    text = re.sub(r'[^a-záéíóúñ\\s]', '', text) # Keep only letters\n",
    "    tokens = word_tokenize(text, language='spanish')\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6146db-8592-44ee-8201-d2ec6ff43b7d",
   "metadata": {
    "id": "3c6146db-8592-44ee-8201-d2ec6ff43b7d"
   },
   "outputs": [],
   "source": [
    "\"\"\"Sentiment Analysis\"\"\"\n",
    "\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the model, tokenizer, and pipeline once\n",
    "stop_words = load_stopwords('spanish')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"ahmedrachid/FinancialBERT-Sentiment-Analysis\", num_labels=3\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0\n",
    ")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        results = sentiment_pipeline(text[:512])[0]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72300d41-ce0d-4fe7-9170-bcb6c0dcc2ef",
   "metadata": {
    "id": "72300d41-ce0d-4fe7-9170-bcb6c0dcc2ef"
   },
   "outputs": [],
   "source": [
    "\"\"\"Data aggregation and analysis\"\"\"\n",
    "def aggregate_sentiment_by_date(df, sentiment_column):\n",
    "    \"\"\"Aggregate sentiment scores by date.\"\"\"\n",
    "    return df.groupby('Date')[sentiment_column].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1133382d-920f-42ff-b657-2d8c53667f53",
   "metadata": {
    "id": "1133382d-920f-42ff-b657-2d8c53667f53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Fetch data\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days=365)\n",
    "stock_data= get_stock_data('^MXX', start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76cb30f2-1190-4212-99f5-084106de5c1b",
   "metadata": {
    "id": "76cb30f2-1190-4212-99f5-084106de5c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 articles.\n"
     ]
    }
   ],
   "source": [
    "# Fetch news articles\n",
    "bolsa_rss_url = \"https://news.google.com/rss/search?q=Bolsa+Mexicana+de+Valores&hl=es-419&gl=MX&ceid=MX:es-419\"\n",
    "news_articles = get_google_news_rss_feed(bolsa_rss_url)\n",
    "if news_articles:\n",
    "    print(f\"Collected {len(news_articles)} articles.\")\n",
    "else:\n",
    "    print(\"No articles were collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6785c1c-a7ce-4b5d-a3e8-a3f7b22f90c4",
   "metadata": {
    "id": "f6785c1c-a7ce-4b5d-a3e8-a3f7b22f90c4"
   },
   "outputs": [
    {
     "ename": "TooManyRequestsError",
     "evalue": "The request failed: Google returned a response with code 429",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fetch google trends\u001b[39;00m\n\u001b[1;32m      2\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcciones\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m trends_data \u001b[38;5;241m=\u001b[39m get_google_trends_data(keywords)\n\u001b[1;32m      4\u001b[0m trends_data\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mget_google_trends_data\u001b[0;34m(keywords, timeframe)\u001b[0m\n\u001b[1;32m      2\u001b[0m pytrends \u001b[38;5;241m=\u001b[39m TrendReq(hl\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes-MX\u001b[39m\u001b[38;5;124m'\u001b[39m, tz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m360\u001b[39m)\n\u001b[1;32m      3\u001b[0m pytrends\u001b[38;5;241m.\u001b[39mbuild_payload(keywords, cat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, timeframe\u001b[38;5;241m=\u001b[39mtimeframe, geo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMX\u001b[39m\u001b[38;5;124m'\u001b[39m, gprop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m pytrends\u001b[38;5;241m.\u001b[39minterest_over_time()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/pytrends/request.py:232\u001b[0m, in \u001b[0;36mTrendReq.interest_over_time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m over_time_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# convert to string as requests will mangle\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtz\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtz\n\u001b[1;32m    229\u001b[0m }\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# make the request and parse the returned json\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m req_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data(\n\u001b[1;32m    233\u001b[0m     url\u001b[38;5;241m=\u001b[39mTrendReq\u001b[38;5;241m.\u001b[39mINTEREST_OVER_TIME_URL,\n\u001b[1;32m    234\u001b[0m     method\u001b[38;5;241m=\u001b[39mTrendReq\u001b[38;5;241m.\u001b[39mGET_METHOD,\n\u001b[1;32m    235\u001b[0m     trim_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    236\u001b[0m     params\u001b[38;5;241m=\u001b[39mover_time_payload,\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    239\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(req_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimelineData\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (df\u001b[38;5;241m.\u001b[39mempty):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/pytrends/request.py:159\u001b[0m, in \u001b[0;36mTrendReq._get_data\u001b[0;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m status_codes\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mtoo_many_requests:\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTooManyRequestsError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mResponseError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "\u001b[0;31mTooManyRequestsError\u001b[0m: The request failed: Google returned a response with code 429"
     ]
    }
   ],
   "source": [
    "keywords = ['BMV', 'Acciones']\n",
    "trends_data = get_google_trends_data(keywords)\n",
    "trends_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253e31a-b8b1-4f57-911d-ed156b832ba0",
   "metadata": {
    "id": "3253e31a-b8b1-4f57-911d-ed156b832ba0"
   },
   "outputs": [],
   "source": [
    "trends_data.reset_index(inplace=True)\n",
    "\n",
    "    # Calcular el cambio porcentual para cada palabra clave\n",
    "for keyword in keywords:\n",
    "    trends_data[f'{keyword}_pct_change'] = trends_data[keyword].pct_change() * 100\n",
    "\n",
    "significant_threshold = 40\n",
    "\n",
    "    # Crear columnas que indiquen si hubo un aumento significativo\n",
    "for keyword in keywords:\n",
    "    trends_data[f'{keyword}_sig_increase'] = trends_data[f'{keyword}_pct_change'].apply(\n",
    "        lambda x: 1 if x > significant_threshold else 0\n",
    "    )\n",
    "trend_sig_columns = [f'{keyword}_sig_increase' for keyword in keywords]\n",
    "trends_data['sentiment_score'] = trends_data[trend_sig_columns].sum(axis=1)\n",
    "\n",
    "trends_data['date'] = pd.to_datetime(trends_data['date']).dt.normalize()\n",
    "trends_data.rename(columns={'date': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb5144-35cf-4a2f-9d5d-d0dfe1f64ead",
   "metadata": {
    "id": "6cfb5144-35cf-4a2f-9d5d-d0dfe1f64ead"
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "client_id=\"fbtWruG8aopQ5chxNLpURw\",\n",
    "client_secret=\"hGgoXi6sA0Qv0wgAezpdXqGvOgc_1Q\",\n",
    "user_agent=\"marketsentimentbmv\"\n",
    ")\n",
    "reddit.read_only = True\n",
    "\n",
    "subreddits = ['MexicoBursatil', 'MexicoFinanciero']\n",
    "query = 'Acciones', 'BMV', 'Comprar', 'Vender', 'Alza', 'Baja'\n",
    "reddit_posts = get_reddit_posts(subreddits, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05228c-2b3b-449c-80c4-efadd3ad3678",
   "metadata": {
    "id": "bc05228c-2b3b-449c-80c4-efadd3ad3678"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "stop_words = load_stopwords('spanish')\n",
    "reddit_df = pd.DataFrame(reddit_posts)\n",
    "\n",
    "reddit_df['Date'] = reddit_df['created'].dt.normalize()\n",
    "reddit_df.drop('created', axis=1, inplace=True)\n",
    "\n",
    "# compute sentiment\n",
    "reddit_df['content_clean'] = reddit_df.apply(lambda x: preprocess_text(x['title'] + ' ' + x['content'], stop_words), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f25260-d989-448e-addc-d31491fb9ee5",
   "metadata": {
    "id": "36f25260-d989-448e-addc-d31491fb9ee5"
   },
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame(news_articles)\n",
    "news_df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "news_df.drop('content', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d9ca9-8523-48a5-af49-5f61f965839a",
   "metadata": {
    "id": "5c2d9ca9-8523-48a5-af49-5f61f965839a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "\n",
    "from_code = \"es\"\n",
    "to_code = \"en\"\n",
    "\n",
    "argostranslate.package.update_package_index()\n",
    "available_packages = argostranslate.package.get_available_packages()\n",
    "package_to_install = next(\n",
    "    filter(\n",
    "        lambda x: x.from_code == from_code and x.to_code == to_code, available_packages\n",
    "    )\n",
    ")\n",
    "argostranslate.package.install_from_path(package_to_install.download())\n",
    "\n",
    "# Translate function\n",
    "reddit_df['translated'] = reddit_df['content_clean'].apply(\n",
    "    lambda x: argostranslate.translate.translate(x, from_code, to_code)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9c287-5ead-4a3d-a0b8-6ba575bfeb03",
   "metadata": {
    "id": "a1f9c287-5ead-4a3d-a0b8-6ba575bfeb03"
   },
   "outputs": [],
   "source": [
    "# Translate function\n",
    "news_df['translated'] = news_df['title'].apply(\n",
    "    lambda x: argostranslate.translate.translate(x, from_code, to_code)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143e617-2ec2-496e-81c8-d859ebb95240",
   "metadata": {
    "id": "b143e617-2ec2-496e-81c8-d859ebb95240"
   },
   "outputs": [],
   "source": [
    "# Get sentiment scores\n",
    "news_df['sentiment_score'] = news_df['translated'].apply(get_sentiment)\n",
    "reddit_df['sentiment_score'] = reddit_df['translated'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08afccf-81f3-429e-bfed-8bd3bd0e67c8",
   "metadata": {
    "id": "b08afccf-81f3-429e-bfed-8bd3bd0e67c8"
   },
   "outputs": [],
   "source": [
    "news_df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "news_df[\"sentiment_score\"] = news_df[\"sentiment_score\"].apply(lambda x: x[\"score\"] if isinstance(x, dict) else x)\n",
    "\n",
    "desired_sentiment = ['Date', 'sentiment_score']\n",
    "news_score_df = news_df[desired_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4061aad-2222-4958-9c74-85d7ca23bf72",
   "metadata": {
    "id": "f4061aad-2222-4958-9c74-85d7ca23bf72"
   },
   "outputs": [],
   "source": [
    "reddit_df[\"sentiment_score\"] = reddit_df[\"sentiment_score\"].apply(lambda x: x[\"score\"])\n",
    "reddit_score_df = reddit_df[desired_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f8624-e3c9-43b7-a2e5-5264288de080",
   "metadata": {
    "id": "0e1f8624-e3c9-43b7-a2e5-5264288de080",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trends_score_df = trends_data[desired_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cc2d2-492f-4c25-8e96-29e9edd51bd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "973cc2d2-492f-4c25-8e96-29e9edd51bd7",
    "outputId": "d87bef26-a83c-4b5e-90ff-ec06602b3e1e"
   },
   "outputs": [],
   "source": [
    "# Calculate stock volatility, momentum\n",
    "stock_data['Returns'] = stock_data['Close'].pct_change()\n",
    "stock_data['Volatility'] = stock_data['Returns'].rolling(window=30).std() * np.sqrt(30)\n",
    "\n",
    "# Fill NAs\n",
    "stock_data['Volatility'].fillna(stock_data['Volatility'].mean(), inplace=True)\n",
    "\n",
    "stock_data['Volatility_90d_avg'] = stock_data['Volatility'].rolling(window=90).mean()\n",
    "stock_data['Volatility_vs_90d_avg'] = stock_data['Volatility'] / stock_data['Volatility_90d_avg']\n",
    "# Calculate volume\n",
    "\n",
    "stock_data['Volume'] = stock_data['Volume'].astype(float)\n",
    "\n",
    "stock_data['Volume_90d_avg'] = stock_data['Volume'].rolling(window=90).mean()\n",
    "stock_data['Volume_vs_90d_avg'] = stock_data['Volume'] / stock_data['Volume_90d_avg']\n",
    "\n",
    "# Calculate momentum\n",
    "\n",
    "stock_data['Momentum'] = stock_data['Close'] - stock_data['Close'].shift(1)\n",
    "stock_data['Momentum_90d_avg'] = stock_data['Momentum'].rolling(window=90).mean()\n",
    "stock_data['Momentum_vs_90d_avg'] = stock_data['Momentum'] / stock_data['Momentum_90d_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4d9ed-e657-4e01-92d9-a8f598a9423a",
   "metadata": {
    "id": "c7f4d9ed-e657-4e01-92d9-a8f598a9423a"
   },
   "outputs": [],
   "source": [
    "stock_desired = ['Date', 'Volatility_vs_90d_avg', 'Volume_vs_90d_avg', 'Momentum_vs_90d_avg']\n",
    "stock_score_df = stock_data[stock_desired]\n",
    "stock_score_df['Volume_vs_90d_avg'].fillna(stock_data['Volume_vs_90d_avg'].mean(), inplace=True)\n",
    "stock_score_df['Momentum_vs_90d_avg'].fillna(stock_data['Momentum_vs_90d_avg'].mean(), inplace=True)\n",
    "stock_score_df['Volatility_vs_90d_avg'].fillna(stock_data['Volatility_vs_90d_avg'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519834a6-f464-4e1e-8c49-d83a35e084ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trends_to_value(score):\n",
    "    if score == 2:\n",
    "        return 1\n",
    "    elif score == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply sentiment conversion functions\n",
    "trends_score_df['sentiment_score'] = trends_score_df['sentiment_score'].apply(trends_to_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeac44d-6f27-4035-862f-2906b8d8d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, dates):\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    dates[\"Date\"] = pd.to_datetime(dates[\"Date\"])\n",
    "    \n",
    "    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    dates[\"Date\"] = dates[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    filtered_df = df[df[\"Date\"].isin(dates[\"Date\"])]\n",
    "    \n",
    "    filtered_df = pd.merge(filtered_df, dates, on=\"Date\", how=\"outer\")\n",
    "\n",
    "    filtered_df = filtered_df.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d77ca-258f-4ca6-b4fe-1b0da73b1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df = pd.DataFrame({\n",
    "    \"Date\": stock_score_df[\"Date\"]\n",
    "})\n",
    "\n",
    "filtered_news = filter_df(news_score_df, dates_df)\n",
    "filtered_reddit = filter_df(reddit_score_df, dates_df)\n",
    "filtered_trends = filter_df(trends_score_df, dates_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be27966-46bf-4f7a-87f5-45c943313a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Date'\n",
    "trends_score_grouped = filtered_trends.groupby('Date')['sentiment_score'].mean().reset_index()\n",
    "news_score_grouped = filtered_news.groupby('Date')['sentiment_score'].mean().reset_index()\n",
    "reddit_score_grouped = filtered_reddit.groupby('Date')['sentiment_score'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05763a-df65-47a7-8926-d80ef2e57086",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_score_grouped['sentiment_score'].fillna(trends_score_grouped['sentiment_score'].mean(), inplace=True)\n",
    "news_score_grouped['sentiment_score'].fillna(news_score_grouped['sentiment_score'].mean(), inplace=True)\n",
    "reddit_score_grouped['sentiment_score'].fillna(reddit_score_grouped['sentiment_score'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e72d4a57-0315-4bc3-a065-2ae06d628084",
   "metadata": {},
   "source": [
    "stock_score_df['Volume_vs_90d_avg'] = np.clip(\n",
    "    stock_score_df['Volume_vs_90d_avg'],\n",
    "    np.percentile(stock_score_df['Volume_vs_90d_avg'], 1),\n",
    "    np.percentile(stock_score_df['Volume_vs_90d_avg'], 99)\n",
    ")\n",
    "\n",
    "stock_score_df['Momentum_vs_90d_avg'] = np.clip(\n",
    "    stock_score_df['Momentum_vs_90d_avg'],\n",
    "    np.percentile(stock_score_df['Momentum_vs_90d_avg'], 1),\n",
    "    np.percentile(stock_score_df['Momentum_vs_90d_avg'], 99)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10849e19-b53d-4acb-b3b8-e47736f127eb",
   "metadata": {
    "id": "10849e19-b53d-4acb-b3b8-e47736f127eb"
   },
   "outputs": [],
   "source": [
    "# Merge all datasets\n",
    "\n",
    "# Merge news and reddit sentiment\n",
    "\n",
    "sentiment_data = pd.merge(\n",
    "    news_score_grouped,\n",
    "    reddit_score_grouped,\n",
    "    on='Date',\n",
    "    how='outer',\n",
    "    suffixes=('_news', '_reddit')\n",
    ")\n",
    "# Then merge with trends\n",
    "sentiment_data = pd.merge(\n",
    "    sentiment_data,\n",
    "    trends_score_grouped,\n",
    "    on='Date',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "sentiment_data['Date'] = pd.to_datetime(sentiment_data['Date'])\n",
    "\n",
    "# Merge financial data with sentiment data\n",
    "\n",
    "sentiment_data = pd.merge(\n",
    "    sentiment_data,\n",
    "    stock_score_df,\n",
    "    on='Date',\n",
    "    how='outer'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efc46a-eb19-4fe0-a539-5b3751e542c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale stock dataa\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_to_normalize = [\n",
    "    'Volatility_vs_90d_avg',\n",
    "    'Volume_vs_90d_avg',\n",
    "    'Momentum_vs_90d_avg',\n",
    "    'sentiment_score_news',\n",
    "    'sentiment_score_reddit',\n",
    "    'sentiment_score',\n",
    "]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sentiment_data[features_to_normalize] = scaler.fit_transform(sentiment_data[features_to_normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba98e3-15f8-49f2-a87f-582a472ea689",
   "metadata": {
    "id": "4fd08a18-ec7a-4358-922b-bdd0051b9fa9"
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'sentiment_score_news': 0.15,\n",
    "    'sentiment_score_reddit': 0.15,\n",
    "    'sentiment_score': 0.10,\n",
    "    'Volatility_vs_90d_avg': 0.25,\n",
    "    'Volume_vs_90d_avg': 0.175,\n",
    "    'Momentum_vs_90d_avg': 0.175,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047d3e7-304c-447b-92a8-a5dbae0d2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(weights.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34ac0c-8671-47db-87c5-3bb793cb55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_index(row, weights):\n",
    "    sentiment_index = 0\n",
    "    for feature, weight in weights.items():\n",
    "        # Directly use the feature value since NaN is already handled\n",
    "        sentiment_index += row[feature] * weight\n",
    "    return sentiment_index * 100  # Scale to 0-1\n",
    "\n",
    "sentiment_data['Sentiment'] = sentiment_data.apply(lambda row: calculate_sentiment_index(row, weights), axis=1)\n",
    "\n",
    "sentiment_index=  sentiment_data[['Date', 'Sentiment']]\n",
    "print(sentiment_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673da7c-a869-4b3a-978e-12f7cbc0fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_index['Sentiment'] = sentiment_index['Sentiment'].ewm(span=7, adjust=False).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0b3df-16b1-4861-a786-28e7af92f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data['ema_sentiment'] = sentiment_data['Sentiment'].ewm(span=7, adjust=False).mean()\n",
    "display(sentiment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f056c94-7632-4cdd-94ac-64944d71a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sentiment_data['Date'], sentiment_data['ema_sentiment'], label='90-Day Moving Average', linestyle='--')\n",
    "plt.title('Sentiment vs. 90-Day Moving Average')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d45098-2802-41ff-8904-a304f110045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_index.to_csv('sentiment_index.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
